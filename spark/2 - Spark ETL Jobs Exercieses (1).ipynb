{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n|facid|Total Slots|\n+-----+-----------+\n|    5|        122|\n|    3|        422|\n|    7|        426|\n|    8|        471|\n|    6|        540|\n|    2|        570|\n|    1|        588|\n|    0|        591|\n|    4|        648|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, lit, sum\n",
    "\n",
    "\n",
    "\n",
    "df_bookings = spark.table(\"bookings\")\n",
    "\n",
    "result = (df_bookings\n",
    "          .filter((to_date(col(\"starttime\"), \"yyyy-mm-dd\") >= to_date(lit(\"2012-09-01\"), \"yyyy-MM-dd\")) & (to_date(col(\"starttime\"), \"yyyy-mm-dd\") <= to_date(lit(\"2012-10-01\"), \"yyyy-MM-dd\")))\n",
    "          .agg(sum(\"slots\").alias(\"Total Slots\"))\n",
    "          .groupBy(col(\"facid\"))\n",
    "          .orderBy(\"Total Slots\"))\n",
    "result.show()\n",
    "\n",
    "result.write.mode(\"overwrite\").parquet(\"/Data/Parquet/Job1\")\n",
    "\n",
    "\n",
    "#dbutils.fs.rm(\"/Data/Parquet/Job1\", recurse=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "df_booking = spark.table(\"bookings\")\n",
    "df_facilities = spark.table(\"members\")\n",
    "df_members = spark.table(\"facilities\")\n",
    "result = df_booking.join(df_members,  df_booking.memid == df_members.memid, \"inner\").join(df_facilities, df_facilities.facid == df_booking.facid, \"inner\").filter(col(\"name\").contains(\"Tennis Court\")).select(concat_ws(\" \",col(\"FIRSTNAME\"),col(\"SURNAME\")).alias(\"Member\"), (col(\"name\").alias(\"Facility\"))).distinct().orderBy(col(\"member\"), col(\"Facility\"))\n",
    "result.show()\n",
    "\n",
    "result.write.mode(\"overwrite\").partitionBy(\"Facility\").saveAsTable(\"threejoin_delta\")\n",
    "\n",
    "spark.sql(\"select * from threejoin_delta\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+\n|Symbol|Week|Max_Close|\n+------+----+---------+\n|  GOOG|  11|    169.0|\n|  GOOG|   3|   197.55|\n|  GOOG|   1|   193.13|\n|  GOOG|   9|   181.19|\n|  GOOG|  12|   166.57|\n|  GOOG|   4|    201.9|\n|  GOOG|   6|   207.71|\n|  GOOG|   5|    205.6|\n|  GOOG|   2|   197.96|\n|  GOOG|  10|   175.75|\n|  GOOG|   7|    188.2|\n|  GOOG|   8|   187.13|\n|  GOOG|  45|   182.28|\n|  GOOG|  44|   176.14|\n|  GOOG|  46|   183.32|\n|  GOOG|  48|   170.82|\n|  GOOG|  43|   166.99|\n|  GOOG|  49|   176.49|\n|  GOOG|  52|   197.57|\n|  GOOG|  47|   179.58|\n+------+----+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType \n",
    "from pyspark.sql.functions import weekofyear, col, max, to_date\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"4f233cc6f1msh55df3814d1f8decp1ba6c2jsn678c171e4410\"\n",
    "}\n",
    "\n",
    "company_list = {\"GOOG\",\"AAPL\",\"MSFT\",\"TSLA\"}\n",
    "stock_data = []\n",
    "\n",
    "for symbol in company_list: \n",
    "    querystring = {\n",
    "        \"function\":\"TIME_SERIES_DAILY\",\n",
    "        \"symbol\":symbol,\n",
    "        \"datatype\":\"json\",\n",
    "        \"outputsize\":\"compact\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "    data = response.json()\n",
    "    time_series = data[\"Time Series (Daily)\"]\n",
    "\n",
    "    for date, details in time_series.items():\n",
    "       # print(f\"Date: {date}, Closing Price: {details}\", symbol)\n",
    "        stock_data.append([\n",
    "                symbol, \n",
    "                date,  \n",
    "                float(details[\"1. open\"]) ,\n",
    "                float(details[\"2. high\"]) ,\n",
    "                float(details[\"3. low\"]),\n",
    "                float(details[\"4. close\"]),\n",
    "                float(details[\"5. volume\"])\n",
    "            ])\n",
    "schema = StructType([\n",
    "    StructField(\"Symbol\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"open\", FloatType(), True),\n",
    "    StructField(\"high\", FloatType(), True),\n",
    "    StructField(\"low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"volume\", FloatType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(stock_data, schema=schema)\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"Week\", weekofyear(col(\"Date\")))\n",
    "\n",
    "\n",
    "df = df.groupBy(\"Symbol\", \"Week\").agg(max(col(\"Close\")).alias(\"Max_Close\"))\n",
    "df.write.partitionBy(\"Symbol\").mode(\"overwrite\").saveAsTable(\"stock_data_table\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\n|      id|          upi|           timestamp|userstamp|           crc64| len|           seq_short|seq_long|                 md5|\n+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\n| 8992498|URS00008936F2| 2015-10-20 18:04:07|   RNACEN|2730EB2FA9B28C9A|1340|GACGAACGCTGGCGGCG...|    null|8618c00bc0151d140...|\n| 8992504|URS00008936F8| 2015-10-20 18:04:07|   RNACEN|82E068BFA7627F08|1377|AGGTCTTCGGACGCTGA...|    null|6220ffbaa3abdf199...|\n| 8992505|URS00008936F9| 2015-10-20 18:04:07|   RNACEN|C67A056BB38B781A|1343|GAGTGGCGAACTGGTGA...|    null|bb3ee68aed9a7f9b5...|\n| 8992510|URS00008936FE| 2015-10-20 18:04:07|   RNACEN|8D38DF2C3ADE58AA|1343|GACGAACGCTGGCGGCG...|    null|861cdda6f680ace2f...|\n| 8992511|URS00008936FF| 2015-10-20 18:04:07|   RNACEN|8F21E0C7591D2A51|1489|ATTGAACGCTGGCGGCA...|    null|ee97ca0d88f4b3d28...|\n|11884232|URS0000B556C8|2017-10-13 16:48:...|   rnacen|85C2F0004D926434| 403|GGGGAATATTGCACAAT...|    null|3f3d8ce96ac13bb88...|\n| 8992514|URS0000893702| 2015-10-20 18:04:07|   RNACEN|5B728705FE5F40E4|1416|GGGTACTTGTACCTGGT...|    null|861e4de8f6765a1a4...|\n| 8992742|URS00008937E6| 2015-10-20 18:04:07|   RNACEN|F59FC7B7779B8779|1329|AGTGGCGAACGGGTGAG...|    null|eec2cd7578730b635...|\n| 8992743|URS00008937E7| 2015-10-20 18:04:07|   RNACEN|A1DE2F08EEFBABED|1476|CAGGACGAACGCTGGCG...|    null|86491ee620b5e079b...|\n| 8992877|URS000089386D| 2015-10-20 18:04:07|   RNACEN|439EE617E89D2CB3|1380|TGAACGCTGGCGGTAGG...|    null|bbabd8de49d9db124...|\n| 8992882|URS0000893872| 2015-10-20 18:04:07|   RNACEN|B562A7547B57ABFD|1390|CTGGCTCAGGATGAACG...|    null|eedd6d6df0c2fe7a0...|\n|12012148|URS0000B74A74|2017-10-13 16:49:...|   rnacen|3BFB675556B33D55| 305|TTGCCCTACGGGGCGCA...|    null|7587defdbd067d528...|\n| 8992883|URS0000893873| 2015-10-20 18:04:07|   RNACEN|FB506FC05CC9E313|1334|CGCTGGCGGCGTGCTTA...|    null|8660a7f7e53b90b82...|\n| 8992888|URS0000893878| 2015-10-20 18:04:07|   RNACEN|3985E719301F9235|1409|GTTAGTGGCGGACGGGT...|    null|bbac7a4a4bd266cf6...|\n| 8992889|URS0000893879| 2015-10-20 18:04:07|   RNACEN|EEE0B60EDE5B2C26|1339|GACGAACGCTGGCGGCG...|    null|62775fc674a037a25...|\n| 8992892|URS000089387C| 2015-10-20 18:04:07|   RNACEN|629EF69F2C43BDB8|1335|CCTGGCTCAGAATGAAC...|    null|bbb09218f8c01b571...|\n| 8992894|URS000089387E| 2015-10-20 18:04:07|   RNACEN|CB89E1ADD8084100|1361|CAGGCCTAACACATGCA...|    null|eede139ea7194a7e8...|\n|11781687|URS0000B3C637|2017-10-13 16:48:...|   rnacen|4CD448E979564764| 550|TAGGGAATCTTCGGCAA...|    null|13a6f992c567f6546...|\n| 8992896|URS0000893880| 2015-10-20 18:04:07|   RNACEN|2313E1DB14FA4495|1448|GCGGCGTGCTTAACACA...|    null|bbb2efbff06d3dd96...|\n| 8992897|URS0000893881| 2015-10-20 18:04:07|   RNACEN|317068CBF65C4297|1343|GACGAACGCTGGCGGCG...|    null|627909a06fce1fe8a...|\n+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresConnection\") \\\n",
    "    .config(\"spark.jars\", \"/path/to/postgresql-connector.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "db_url = \"jdbc:postgresql://hh-pgsql-public.ebi.ac.uk:5432/pfmegrnargs\"\n",
    "db_properties = {\n",
    "    \"user\": \"reader\",\n",
    "    \"password\": \"NWDMCE5xdipIjRrp\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(url=db_url, table=\"rna\", properties=db_properties)\n",
    "df = df.limit(100)\n",
    "df.show()\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"rna_100_records\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}